# -*- coding: utf-8 -*-
"""image_caption.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AIFwnFsf8Vc72uR5pow_FufOrs1EMMwW
"""

import os
os.environ['KERAS_BACKEND'] = 'tensorflow'
import re
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import keras
from keras import layers
from keras.applications import efficientnet
from keras.layers import TextVectorization
from tensorflow.keras.layers import TimeDistributed

keras.utils.set_random_seed(111)

"""define constants"""

DATASET_PATH=r'flickr30k'
IMAGE_SIZE=299,299
VOCAB_SIZE=10000
BATCH_SIZE=64
SEQ_LENGTH=25
EMBED_DIM=512
EPOCHS=30
FF_DIM=512
AUTOTUNE=tf.data.AUTOTUNE

"""Preparing the dataset"""

def load_captions_data(filename):
    """
    Loads captions (text) data and maps them to corresponding images.

    Args:
        filename: Path to the text file containing caption data.

    Returns:
        caption_mapping: Dictionary mapping image names and the corresponding captions
        text_data: List containing all the available captions
    """
    with open(filename) as caption_file:
        caption_data = caption_file.readlines()
    caption_mapping = {}
    text_data = []
    images_to_skip = set()
    for line in caption_data:
        line = line.rstrip('\n')
        if index:=line.find('"')!=-1:
          line=line[:index]+line[index+1:]
        if line == '':
            continue
        if '.jpg, ' not in line:
            continue
        img_name, caption = line.split('.jpg, ')
        img_name = os.path.join(DATASET_PATH+'/Images', img_name+'.jpg')
        tokens = caption.strip().split()
        if len(tokens) < 5 or len(tokens) > SEQ_LENGTH:
            images_to_skip.add(img_name)
            continue
        if img_name.endswith('.jpg'):
            caption = '<start> ' + caption.strip() + ' <end>'
            text_data.append(caption)
            if img_name in caption_mapping:
                caption_mapping[img_name].append(caption)
            else:
                caption_mapping[img_name] = [caption]
    for img_name in images_to_skip:
        if img_name in caption_mapping:
            del caption_mapping[img_name]
    return caption_mapping, text_data

def train_val_split(caption_data,train_size=0.8,shuffle=True):
  """
  Split the captioning dataset into train and validation sets.

    Args:
        caption_data (dict): Dictionary containing the mapped caption data
        train_size (float): Fraction of all the full dataset to use as training data
        shuffle (bool): Whether to shuffle the dataset before splitting

    Returns:
        Traning and validation datasets as two separated dicts
    """
  all_images=list(caption_data.keys())
  if shuffle:
    np.random.shuffle(all_images)
  train_size=int(len(caption_data)*train_size)
  train_data={img_name:caption_data[img_name]for img_name in all_images[:train_size]}
  validation_data={img_name:caption_data[img_name]for img_name in all_images[train_size:]}
  return train_data,validation_data

captions_mapping,text_data=load_captions_data(DATASET_PATH+r'/captions.txt')
train_data,valid_data=train_val_split(captions_mapping)
print("Number of training samples: ", len(train_data))
print("Number of validation samples: ", len(valid_data))

print(list(captions_mapping))

"""Vectorizing the text data"""

def custom_standardization(input_string):
    lowercase = tf.strings.lower(input_string)
    return tf.strings.regex_replace(lowercase, "[%s]" % re.escape(strip_chars), "")

strip_chars = "!\"#$%&'()*+,-./:;<=>?@[\]^_`{|}~"
strip_chars = strip_chars.replace("<", "")
strip_chars = strip_chars.replace(">", "")
vectorization=TextVectorization(
    max_tokens=VOCAB_SIZE,
    output_mode="int",
    output_sequence_length=SEQ_LENGTH,
    standardize=custom_standardization,
)
vectorization.adapt(text_data)
image_augmentation = keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.1),
])

"""Building a tf.data.Dataset pipeline for training

"""

def decode_and_resize(img_path):
    img = tf.io.read_file(img_path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, IMAGE_SIZE)   # IMAGE_SIZE should be (299, 299)
    img = tf.image.convert_image_dtype(img, tf.float32)
    return img

def process_input(img_path,captions):
  return decode_and_resize(img_path),vectorization(captions)
def make_dataset(images, captions):
    all_image_paths = []
    all_captions = []

    for img_path, cap_list in zip(images, captions):
        for cap in cap_list[:5]:  # limit to 5 captions
            all_image_paths.append(img_path)
            all_captions.append(cap)

    # Create dataset from single examples (no batching yet)
    ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_captions))

    # Shuffle before map
    ds = ds.shuffle(buffer_size=1000)

    # Map preprocessing on each single example (img_path, caption)
    ds = ds.map(process_input, num_parallel_calls=AUTOTUNE)

    # Now batch the dataset to desired batch size
    ds = ds.batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)

    return ds


train_dataset=make_dataset(list(train_data.keys()),list(train_data.values()))
valid_dataset=make_dataset(list(valid_data.keys()),list(valid_data.values()))

"""build the model"""

def get_cnn_model():
    base_model = efficientnet.EfficientNetB0(
        input_shape=(*IMAGE_SIZE, 3),
        include_top=False,
        weights="imagenet",
    )
    # We freeze our feature extractor
    base_model.trainable = False
    base_model_out = base_model.output
    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)
    cnn_model = keras.models.Model(base_model.input, base_model_out)
    return cnn_model


class TransformerEncoderBlock(layers.Layer):
    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.dense_dim = dense_dim
        self.num_heads = num_heads
        # Multi-head attention allows the model to focus on different parts of the input (like image patches or words)
        # from multiple perspectives at the same time, helping it understand relationships and context better.
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, #number of parallels to focus on
            key_dim=embed_dim, # each "pair of eyes" can see everything.
            dropout=0.0 # nobody of the sensors takes a break
        )
        self.layernorm_1 = layers.LayerNormalization()# keeps everything balanced
        self.layernorm_2 = layers.LayerNormalization()# keeps everything balanced
        self.dense_1 = layers.Dense(embed_dim, activation="relu") #it takes some numbers, mixes them up, and spits out new numbers, keeps the numbers that above 0

    def call(self, inputs, training, mask=None):
        inputs = self.layernorm_1(inputs) # It makes sure everything is neat and tidy so your brain can focus better.
        inputs = self.dense_1(inputs)# it helps your brain get ready to think about the story more clearly.
        # Now your brain uses its special superpower called “attention” to look carefully at every part of the story.
        #It’s like shining a flashlight on the most important parts, so you don’t miss anything.
        attention_output_1 = self.attention_1(
            query=inputs,
            value=inputs,
            key=inputs,
            attention_mask=None,
            training=training,
        )
        out_1 = self.layernorm_2(inputs + attention_output_1) # You take the original story plus the important stuff your brain found and put them together neatly.
        return out_1


class PositionalEmbedding(layers.Layer):
    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):
        super().__init__(**kwargs)
        # Think of this like a big box of colorful stickers, one sticker for each word in your secret code (your vocabulary).
        # When the computer sees a word, it picks the matching sticker that shows what that word means in colors and shapes.
        # This helps the computer understand words as numbers it can work with.
        self.token_embeddings = layers.Embedding(
            input_dim=vocab_size, output_dim=embed_dim
        )
        #imagine your secret code is a sentence, and order matters (first word, second word, etc.).
        #These are special stickers that tell the computer where each word is in the sentence — like numbering your LEGO blocks so you know which comes first.
        #It helps the computer know the order of words.
        self.position_embeddings = layers.Embedding(
            input_dim=sequence_length, output_dim=embed_dim
        )
        self.sequence_length = sequence_length
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        #This is like making the stickers brighter or dimmer so the computer can “see” them better.
        #It makes the colors and shapes easier to understand by scaling them up just right.
        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))

    def call(self, inputs):
        length = tf.shape(inputs)[-1]
        positions = tf.range(start=0, limit=length, delta=1) # give a range of positions
        embedded_tokens = self.token_embeddings(inputs)
        embedded_tokens = embedded_tokens * self.embed_scale # make the numbers more clear to the model
        embedded_positions = self.position_embeddings(positions)
        return embedded_tokens + embedded_positions

    def compute_mask(self, inputs, mask=None):
        return tf.math.not_equal(inputs, 0)


class TransformerDecoderBlock(layers.Layer):
    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.ff_dim = ff_dim
        self.num_heads = num_heads
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.1 #Dropout = 0.1 means that the computer is covering 1 out of every 10 fingers (or brain cells!)
        )
        self.attention_2 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.1 # Dropout = 0.1 means that the computer is covering 1 out of every 10 fingers (or brain cells!)
        )
        self.ffn_layer_1 = layers.Dense(ff_dim, activation="relu")
        self.ffn_layer_2 = layers.Dense(embed_dim)
        # Keeps everything clean, balanced, and easy to understand — so the model can learn smoothly
        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.layernorm_3 = layers.LayerNormalization()
        # gives each word or piece a “position tag”
        self.embedding = PositionalEmbedding(
                        embed_dim=EMBED_DIM,         # How big each word or patch’s information is
                        sequence_length=SEQ_LENGTH,  # How many words or pieces we have in total
                        vocab_size=VOCAB_SIZE        # How many different words we know
        )
        self.out = layers.Dense(VOCAB_SIZE, activation="softmax")
        # The model randomly hides 30% of its little helpers (neurons) during learning.
        # This helps the model not rely too much on just a few helpers and forces it to learn better, more balanced ways.
        self.dropout_1 = layers.Dropout(0.3)
        self.dropout_2 = layers.Dropout(0.5)
        self.supports_masking = True #  means the layer can skip over empty spots in input sequences, helping the model learn better with variable-length data

    def call(self, inputs, encoder_outputs, training, mask=None):
        inputs = self.embedding(inputs)
        causal_mask = self.get_causal_attention_mask(inputs)# This function creates a mask that prevents each word from seeing future words in the sequence, repeated for every example in the batch.

        if mask is not None: # if a padding mask was provided
        #these lines are for ignoring empty spaces and save time
            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32) # add a dimension and do a cast for true and flase to 1 and 0
            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32) # add a dimension and do a cast for true and flase to 1 and 0
            combined_mask = tf.minimum(combined_mask, causal_mask)

        # Use attention to help each word understand its meaning better
        # by looking at the other words in the sentence.

        attention_output_1 = self.attention_1(
            query=inputs,                 # The word that's asking: "Who should I pay attention to?"
            value=inputs,                 # The actual content or meaning of each word
            key=inputs,                   # The keys that help decide how important each word is
            attention_mask=combined_mask, # Tells the model: "Don't look at padding or future words!"
            training=training             # Whether the model is training or just using what it learned
        )

        out_1 = self.layernorm_1(inputs + attention_output_1)

        attention_output_2 = self.attention_2(
            query=out_1,
            value=encoder_outputs,
            key=encoder_outputs,
            attention_mask=padding_mask,
            training=training,
        )
        out_2 = self.layernorm_2(out_1 + attention_output_2)

        ffn_out = self.ffn_layer_1(out_2)
        ffn_out = self.dropout_1(ffn_out, training=training)
        ffn_out = self.ffn_layer_2(ffn_out)

        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)
        ffn_out = self.dropout_2(ffn_out, training=training)
        preds = self.out(ffn_out)
        return preds

    def get_causal_attention_mask(self, inputs):
        input_shape = tf.shape(inputs)# get the shapes of the tensor
        batch_size, sequence_length = input_shape[0], input_shape[1]
        i = tf.range(sequence_length)[:, tf.newaxis]# create a column of numbers between  to sequence_length-1
        j = tf.range(sequence_length)# create a row of numbers between  to sequence_length-1
        mask = tf.cast(i >= j, dtype="int32")
        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))
        mult = tf.concat(
            [
                tf.expand_dims(batch_size, -1),   # How many copies to make
                tf.constant([1, 1], dtype=tf.int32), # Don’t copy in these directions
            ],
            axis=0, # puts these numbers together into one list.
        )

        return tf.tile(mask, mult) # make the original small mask (tower) and make batch_size copies of it stacked in the first direction


class ImageCaptioningModel(keras.Model):
    def __init__(
        self,
        cnn_model,
        encoder,
        decoder,
        num_captions_per_image=5,
        vectorizer=None,
        image_aug=None,
    ):
        super().__init__()
        self.cnn_model = cnn_model
        self.encoder = encoder
        self.decoder = decoder
        self.loss_tracker = keras.metrics.Mean(name="loss") # is like counting how many mistakes model made.
        self.acc_tracker = keras.metrics.Mean(name="accuracy") # is like counting how many times model got it right.

        self.num_captions_per_image = num_captions_per_image
        self.image_aug = image_aug
        self.vectorizer = vectorizer
    def calculate_loss(self, y_true, y_pred, mask):
        # These lines make sure we only count errors on the important parts and skip the empty spaces."
        loss = self.loss(y_true, y_pred)
        mask = tf.cast(mask, dtype=loss.dtype)# caonvert the mask to the tyoe of loss
        loss *= mask # Only count the mistakes on the real questions, ignore the blank ones
        return tf.reduce_sum(loss) / tf.reduce_sum(mask) # Average mistakes per real question

    def calculate_accuracy(self, y_true, y_pred, mask):
        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2)) # compare the y_predict to the y_true
        accuracy = tf.math.logical_and(mask, accuracy)# ignore the blank spots
        accuracy = tf.cast(accuracy, dtype=tf.float32)# convert to 1 and 0
        mask = tf.cast(mask, dtype=tf.float32)# convert to 1 and 0
        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask) # percentage of questions you got right, ignoring blanks, the scores/probabilities for all possible classes (words) are in axis 2

    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):
        # Encode the image embeddings to get meaningful features
        encoder_out = self.encoder(img_embed, training=training)
        # Prepare input captions by removing the last token (for teacher forcing)
        batch_seq_inp = batch_seq[:, :-1]
        # Prepare target captions by removing the first token (the true next words)
        batch_seq_true = batch_seq[:, 1:]
        # Create a mask to ignore padding tokens (assumed to be 0) during loss/accuracy calculation
        mask = tf.math.not_equal(batch_seq_true, 0)
        # Use the decoder to predict the next word tokens given the inputs and encoded image
        batch_seq_pred = self.decoder(
            batch_seq_inp, encoder_out, training=training, mask=mask
        )
        # Calculate the loss between predicted and true captions, ignoring padding
        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)
        # Calculate the accuracy of the predicted captions compared to the true captions
        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)
        # Return the loss and accuracy values for this batch
        return loss, acc

    def train_step(self, batch_data):
      batch_img, batch_caption = batch_data

      print("Before augmentation:", batch_img.shape)

      # Apply augmentation if available
      if self.image_aug:
         batch_img = self.image_aug(batch_img)

      # 1. Get image embeddings from encoder (CNN)
      img_embed = self.cnn_model(batch_img)

      # 2. Vectorize captions: (batch_size, seq_len)
      batch_seq = batch_caption # or use vectorization(batch_caption) if defined outside

      # 3. Compute loss and accuracy
      # Start recording operations for automatic differentiation (to compute gradients)
      with tf.GradientTape() as tape:

          # Compute the loss and accuracy for the current batch of image-caption pairs
          # This includes running the image through the encoder and generating predictions with the decoder
          loss, acc = self._compute_caption_loss_and_acc(
              img_embed, batch_seq, training=True
          )


      # 4. Get trainable variables
      train_vars = self.encoder.trainable_variables + self.decoder.trainable_variables

      # 5. Apply gradients
      # Compute gradients of the loss with respect to the model's trainable variables
      grads = tape.gradient(loss, train_vars)
      # Apply the gradients to update the model weights using the optimizer
      self.optimizer.apply_gradients(zip(grads, train_vars))


      # 6. Update trackers
      self.loss_tracker.update_state(loss)
      self.acc_tracker.update_state(acc)

      # 7. Return metrics
      return {
         "loss": self.loss_tracker.result(),
          "acc": self.acc_tracker.result(),
      }

    def test_step(self, batch_data):
        batch_img, batch_seq = batch_data
        batch_loss = 0
        batch_acc = 0

        # 1. Get image embeddings
        img_embed = self.cnn_model(batch_img)

        # 2. Pass each of the five captions one by one to the decoder
        # along with the encoder outputs and compute the loss as well as accuracy
        # for each caption.
        for i in range(self.num_captions_per_image):
            loss, acc = self._compute_caption_loss_and_acc(img_embed, batch_seq, training=False)
            # 3. Update batch loss and batch accuracy
            batch_loss += loss
            batch_acc += acc
        batch_acc /= float(self.num_captions_per_image)
        # 4. Update the trackers
        self.loss_tracker.update_state(batch_loss)
        self.acc_tracker.update_state(batch_acc)
        # 5. Return the loss and accuracy values
        return {"loss": self.loss_tracker.result(),"acc": self.acc_tracker.result()}

    @property
    def metrics(self):
        # We need to list our metrics here so the `reset_states()` can be
        # called automatically.
        return [self.loss_tracker, self.acc_tracker]


cnn_model = get_cnn_model()
encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=1)
decoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=2)
caption_model = ImageCaptioningModel(
    cnn_model=cnn_model,
    encoder=encoder,
    decoder=decoder,
    vectorizer=vectorization,
    image_aug=image_augmentation,
)

"""model training"""

# Define the loss function that compares predicted class probabilities to true class labels.
# 'from_logits=False' means the predictions are already passed through softmax.
# 'reduction=None' means we'll manually handle how we average the losses later.
cross_entropy = keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction=None)
# Create an early stopping callback that stops training if the model hasn't improved for 3 epochs.
# It also restores the model weights from the best-performing epoch.
early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)

class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):
  def __init__(self,post_warmup_learning_rate,warmup_steps):
    super().__init__()
    self.post_warmup_learning_rate=post_warmup_learning_rate
    self.warmup_steps=warmup_steps
  def __call__(self,step):
    global_step=tf.cast(step,tf.float32)
    warmup_steps=tf.cast(self.warmup_steps,tf.float32)
    warmup_progress=global_step/warmup_steps
    warmup_learning_rate=self.post_warmup_learning_rate*warmup_progress
    # Return the warmup learning rate if we haven't passed the warmup steps yet,
    # otherwise return the normal post-warmup learning rate.
    return tf.cond(
        global_step < warmup_steps,                # Condition to check: are we still warming up?
        lambda: warmup_learning_rate,              # If YES: use the smaller warmup learning rate
        lambda: self.post_warmup_learning_rate     # If NO: use the normal learning rate
    )


num_train_steps=len(train_dataset)*EPOCHS
num_warmup_steps=num_train_steps//15
lr_schedule=LRSchedule(post_warmup_learning_rate=1e-4,warmup_steps=num_warmup_steps)
caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule),loss=cross_entropy)
caption_model.fit(train_dataset,epochs=EPOCHS,validation_data=valid_dataset,callbacks=[early_stopping],)
caption_model.save('caption_model')

vocab=vectorization.get_vocabulary()
index_lookup=dict(zip(range(len(vocab)),vocab))
max_decoded_sentence_length = SEQ_LENGTH - 1
valid_images = list(valid_data.keys())

# Load the saved caption model
caption_model = keras.models.load_model("caption_model")

def generate_caption(valid_images, decode_and_resize, vectorization, index_lookup, max_decoded_sentence_length=40):
    # 1. Select a random image from the validation set
    sample_img = np.random.choice(valid_images)

    # 2. Read and resize the image
    sample_img = decode_and_resize(sample_img)
    img = sample_img.numpy().clip(0, 255).astype(np.uint8)
    plt.imshow(img)
    plt.axis("off")
    plt.show()

    # 3. Prepare image for the model (add batch dimension)
    img = tf.expand_dims(sample_img, 0)

    # 4. Pass through CNN encoder
    img = caption_model.cnn_model(img)

    # 5. Encode image features
    encoded_img = caption_model.encoder(img, training=False)

    # 6. Start building the caption
    decoded_caption = "<start> "
    for i in range(max_decoded_sentence_length):
        tokenized_caption = vectorization([decoded_caption])[:, :-1]
        mask = tf.math.not_equal(tokenized_caption, 0)

        # 7. Predict next word
        predictions = caption_model.decoder(
            tokenized_caption, encoded_img, training=False, mask=mask
        )
        sampled_token_index = np.argmax(predictions[0, i, :])
        sampled_token = index_lookup[sampled_token_index]

        # 8. Stop if <end> token is predicted
        if sampled_token == "<end>":
            break
        decoded_caption += " " + sampled_token

    # 9. Clean and print the final caption
    decoded_caption = decoded_caption.replace("<start> ", "")
    decoded_caption = decoded_caption.replace(" <end>", "").strip()
    print("Predicted Caption:", decoded_caption)